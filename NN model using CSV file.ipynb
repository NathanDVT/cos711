{"cells":[{"metadata":{},"cell_type":"markdown","source":"Hai Everyone. This is a simple model for classification. \n\n**STEPS**\n\n1]  Importing required libraries\n\n2] Importing the data\n\n3] Data pre-processing\n\n4] creating a model\n\n5] Fitting the model\n"},{"metadata":{},"cell_type":"markdown","source":"**Step 1 **: Importing the required libraries."},{"metadata":{"_cell_guid":"170f0fb3-b5da-4a9b-870a-0365b4bc8f30","_uuid":"a31348a5eba01332c755b5d244ff9d8a5f1d3ee7","scrolled":true,"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful anal,ytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.layers import Flatten\nfrom keras.layers.convolutional import Convolution2D\nfrom keras.layers.convolutional import MaxPooling2D\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder\nfrom keras import backend as K\nfrom tqdm import tqdm\nimport tensorflow as tf\n\nimport math\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"98fd2558-1557-4d4b-a849-a65feaa821fa","_uuid":"f9a7e29aaaa6189886df0b3ac01cdaa1509c6d25"},"cell_type":"markdown","source":"**Step 2** :As mentioned above the input file present in  \"../input/\". We import it using pandas"},{"metadata":{"trusted":true},"cell_type":"code","source":"train=pd.read_csv(\"../input/ziradata/ReducedTrain.csv\")\ntest= pd.read_csv(\"../input/ziradata/ReducedTest.csv\")\ntrain.head()\ndef replace_nan(x):\n    if x==\" \":\n        return np.nan\n    else :\n        return float(x)\nfeatures=[\"temp\",\"precip\",\"rel_humidity\",\"wind_dir\",\"wind_spd\",\"atmos_press\"]\nfor feature in features : \n    train[feature]=train[feature].apply(lambda x: [ replace_nan(X) for X in x.replace(\"nan\",\" \").split(\",\")])\n    test[feature]=test[feature].apply(lambda x: [ replace_nan(X)  for X in x.replace(\"nan\",\" \").split(\",\")]) \ndef aggregate_features(x,col_name):\n    x[\"max_\"+col_name]=x[col_name].apply(np.max)\n    x[\"min_\"+col_name]=x[col_name].apply(np.min)\n    x[\"mean_\"+col_name]=x[col_name].apply(np.mean)\n    x[\"std_\"+col_name]=x[col_name].apply(np.std)\n    x[\"var_\"+col_name]=x[col_name].apply(np.var)\n    x[\"median_\"+col_name]=x[col_name].apply(np.median)\n    x[\"ptp_\"+col_name]=x[col_name].apply(np.ptp)\n    return x  \ndef remove_nan_values(x):\n    return [e for e in x if not math.isnan(e)]\ndata = pd.concat([train,test]).reset_index(drop=True)\ndata.columns.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col_name in tqdm(features):\n    data[col_name]=data[col_name].apply(remove_nan_values)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"64a5d464-ddfe-4463-be41-f80d08bcf020","_uuid":"b620edee633b53fea2896cda5f42acbb128c00a6","trusted":true},"cell_type":"code","source":"xoxo = 0\nfor x in range(121):\n    # print(data.values)\n    # data[\"location\"+ str(x)] = ord(data[\"location\"]) - 65\n    data[\"newtemp\"+ str(x)] = data.temp.str[x]\n    data[\"newprecip\"+ str(x)] = data.precip.str[x]\n    data[\"newrel_humidity\"+ str(x)] = data.rel_humidity.str[x]\n    data[\"newwind_dir\"+ str(x)] = data.wind_dir.str[x]\n    data[\"windspeed\"+ str(x)] = data.wind_spd.str[x]\n    data[\"atmospherepressure\"+ str(x)] = data.atmos_press.str[x]\ndata = data.drop(['location','ID',\"temp\",\"precip\",\"rel_humidity\",\"wind_dir\",\"wind_spd\",\"atmos_press\"], axis=1)\n\ntrain=data[data.target.notnull()].reset_index(drop=True)\ntest=data[data.target.isnull()].reset_index(drop=True)\n# Read training and test data files\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Step3**:\nThis is important step becaues the keras function accepts the inputs as images.\nBut here we are using CSV file which is a array. So at first we have to convert these into image(All images are the 3-D matrix of pixels).\nSo now we re-shape the matrix into a 3-D matrix.we separate the X(input) and Y(output) from Data\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reshape and normalize training data\n\ntrainX = train.values[:, 1:].reshape(train.shape[0],1,121, 6).astype( 'float32' )\nX_train = trainX / 100.0\n\ny_train = train.values[:,0]\n\n\n# Reshape and normalize test data\ntestX = test.values[:,1:].reshape(test.shape[0],1, 121, 6).astype( 'float32' )\nX_test = testX / 100.0\n\ny_test = test.values[:,0]\ntrainX.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"80cb894d-76c4-4494-a4ac-5e3875852ab0","_uuid":"ad7053bca185de080b340a000d753b8c9fade0ff"},"cell_type":"markdown","source":"The model always gives a matrix of probability as output. The max value in that will be the output label. so the next process of data pre-processing is  to encode the output.\nAs I said above the output will be in a matrix of 10 values. so we will define 10 neurons in the output layer.\nBut our csv file contains single values. so we enode the values.\nExample: In the classification of cat(0), dog(1) and monkey(2).The output of the model will look like this [ 0.23 , 0.01 , 0.91 ].\nBut in csv file it will be a single digit i.e 2.\nso we encode  2 as [ 0 , 0 , 1], 1 as [ 0 , 1 , 0] and 0 as [ 1 , 0 , 0 ].  \n\nThis is done by LabelBinarizer() function.\n"},{"metadata":{"_cell_guid":"04bd1d50-0dee-409d-938e-678bcfb5a944","_uuid":"ba10ccfb03cf8b78ad5b0672cb1cba91d3afb8f1","scrolled":true,"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\nlb = preprocessing.LabelBinarizer()\nX_train.shape\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0cf07c87-6c6d-4ec4-8d5d-ae22257a1408","_uuid":"4ef20bcc50040ebebc16ca151f00afd23f24709e"},"cell_type":"markdown","source":"**Step 4:**\nNow we build a model.\nThe model has 2 Convolution layer 2 MaxPooling layer and 2 hidden layers.\nThe flatten mainly used to flatten the 3-D array of previous layer into a single layer(Because the Ann model only take 1-D array as input).So the flatten creates the input layer.\nThere are 10 labels so there will be 10 neurons in the output layer.\nThere are 784 columns(Pixels).But we reshaped it to 28*28.\nSo the Convolution layer input_shape will be (1,28,28) .\nDropout() is used to avoid over-fitting.\n"},{"metadata":{"_cell_guid":"a8a37043-21a4-4cc3-9541-eb44b51a7fcf","_uuid":"d8eba0003c9abe716658aa09a9e432474bec94a7","scrolled":true,"trusted":true},"cell_type":"code","source":"model = Sequential()\nK.set_image_dim_ordering('th')\nmodel.add(Convolution2D(30, 3, 3, border_mode= 'valid' , input_shape=( 1, 121, 6),activation= 'relu' ))\nmodel.add(MaxPooling2D(pool_size=(1, 1)))\nmodel.add(Convolution2D(15, 3,3, activation= 'relu' ))\nmodel.add(MaxPooling2D(pool_size=(1, 1)))\nmodel.add(Dropout(0.1))\nmodel.add(Flatten())\nmodel.add(Dense(600, activation= 'sigmoid' ))\nmodel.add(Dense(50, activation= 'sigmoid' ))\nmodel.add(Dense(1, kernel_initializer='normal'))\n  # Compile model\nmodel.compile(loss= 'mean_squared_error' , optimizer= 'adam' , metrics=[ 'accuracy' ])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"afab15f4-1174-4ba7-9d7f-cd565a487f0b","_uuid":"9473c31c5375c8e152ddcf85005c1f6d71132eb8"},"cell_type":"markdown","source":"**Step 5 **:\nNow we fit the model with the data.\nYou can increase the accuracy by increasing the number of epochs , Conv layer , Maxpool layers.(In this case).\nBut if you are delaing with images  then  best way is to use Image Data Generator.\nThis uses  real-time data augmentation to produce wide varity of images for a same label.The main advantage of this is the amount of Data is reduced."},{"metadata":{"_cell_guid":"2943f123-0fb5-49ce-b0ab-eeffc4e9f4f0","_uuid":"5818621dd52ea1d7ffe5c390acb6672b68afccc6","trusted":true},"cell_type":"code","source":"model.fit(X_train, y_train,\n          epochs=500,\n          batch_size= 70)\nscore = model.evaluate(X_train, y_train, batch_size=70)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If you want to see the summary of model then use summary method ."},{"metadata":{"_cell_guid":"f454bf85-8c47-496d-93e6-486493264864","_uuid":"fb13ad26cdf0ccb772c022825a5393a409e1a776","trusted":true},"cell_type":"code","source":"model.summary()\n\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}